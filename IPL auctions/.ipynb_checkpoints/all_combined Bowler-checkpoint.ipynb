{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported files\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import os.path\n",
    "print 'imported files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc2011 = pd.read_csv('2011_auctions.csv')\n",
    "pnames= []\n",
    "players = auc2011['player_name']\n",
    "for play in players:\n",
    "    pnames.append(str(play.replace(' ','+')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for player_name in pnames:\n",
    "    try:\n",
    "\n",
    "        fname = 'score_files/2016b_t20_' + str(player_name) + '.csv'\n",
    "        print fname\n",
    "        print player_name\n",
    "        if (os.path.isfile(fname)):\n",
    "            print 'file exists'\n",
    "            continue\n",
    "\n",
    "        url = 'http://www.bing.com/search?q=cricketarchive.com+player+' + player_name\n",
    "        print url\n",
    "        #url  ='http://www.bing.com/search?q=cricketarchive.com+player+ms+dhoni'\n",
    "        opener = urllib2.build_opener()\n",
    "        opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        page = opener.open(url)\n",
    "        soup = BeautifulSoup(page.read(),\"html.parser\")\n",
    "        print 'bing sorted'\n",
    "        #finding list a and t20 urls\n",
    "        divs = soup.findAll('div',{'class':'b_attribution'})\n",
    "        for div in divs:\n",
    "            print 'in div bhai'\n",
    "            print div.text\n",
    "\n",
    "            if str(div.text).find('cricketarchive.com/Archive/Players') != -1 or str(div.text).find('cricketarchive.com/Players')!= -1 :\n",
    "\n",
    "\n",
    "                #__________________MAKE CHANGES SOME HAS ARCHIVE SOME DONT IN URL______________________\n",
    "                if str(div.text).find('cricketarchive.com/Archive/Players') != -1:\n",
    "                    types= 1\n",
    "\n",
    "                else:\n",
    "                    types = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print div.text\n",
    "                s1 = div.text\n",
    "                s2 = 'Players'\n",
    "                req_str =  s1[s1.index(s2) + len(s2):]\n",
    "                wrds =  req_str.split('/')\n",
    "                final_str = wrds[1]+ '/' + wrds[2]\n",
    "                #print final_str\n",
    "                if types == 1:\n",
    "                    listA_url = str('http://cricketarchive.com/Archive/Players/'+final_str+'/List_A_Matches.html')\n",
    "                    t20_url =   str('http://cricketarchive.com/Archive/Players/'+final_str+'/Twenty20_Matches.html')\n",
    "                else:\n",
    "\n",
    "                    listA_url = str('http://cricketarchive.com/Players/'+final_str+'/List_A_Matches.html')\n",
    "                    t20_url =   str('http://cricketarchive.com/Players/'+final_str+'/Twenty20_Matches.html')\n",
    "\n",
    "                break\n",
    "        #urls found out\n",
    "\n",
    "        listA_runs = []\n",
    "        t20_runs = []\n",
    "        listA_overs = []\n",
    "        t20_overs = []\n",
    "        listA_wkt = []\n",
    "        t20_wkt = []\n",
    "        \n",
    "        listA_date = []\n",
    "        t20_date = []\n",
    "\n",
    "\n",
    "        page = opener.open(listA_url)\n",
    "        soup = BeautifulSoup(page.read(),\"html.parser\")\n",
    "        table = soup.findAll('table')\n",
    "\n",
    "\n",
    "        rows =  table[0].findAll('tr')\n",
    "        #print rows[0]\n",
    "        date_start  = '2015-02-04'\n",
    "        date_end  = '2016-02-04'\n",
    "\n",
    "        for row in rows:\n",
    "            tds  = row.findAll('td')\n",
    "            #print tds[1].text\n",
    "            date  = parser.parse(str(tds[1].text))\n",
    "            if date > parser.parse(date_start) and date < parser.parse(date_end):\n",
    "                print date.date()\n",
    "                #listA_date.append(str(date.date()))\n",
    "                mlink = tds[4].find('a')\n",
    "                match_link = 'http://cricketarchive.com/' + mlink['href']\n",
    "                print match_link\n",
    "                match_page = opener.open(match_link)\n",
    "                match_soup = BeautifulSoup(match_page.read(),\"html.parser\")\n",
    "                #print match_soup\n",
    "\n",
    "                found  = 0\n",
    "                tabs = match_soup.findAll('table')\n",
    "\n",
    "\n",
    "                rows = tabs[2].findAll('tr')\n",
    "                for row in rows:\n",
    "                    tds  = row.findAll('td')\n",
    "                    if len(tds)  < 4:\n",
    "                        continue\n",
    "                    if tds[0].find('a'):\n",
    "                        link_text =  tds[0].find('a')['href']\n",
    "                        if link_text.find(final_str) != -1:\n",
    "                            #print tds[2].text\n",
    "                            #print tds[3].text\n",
    "                            listA_runs.append(tds[3].text)\n",
    "                            listA_wkt.append(tds[4].text)\n",
    "                            listA_overs.append(tds[1].text)\n",
    "                            found =  1\n",
    "                            listA_date.append(str(date.date()))\n",
    "                            break\n",
    "                if (found== 1 ):\n",
    "                    continue\n",
    "\n",
    "\n",
    "                rows = tabs[4].findAll('tr')\n",
    "                for row in rows:\n",
    "                    tds  = row.findAll('td')\n",
    "                    if len(tds) < 4:\n",
    "                        continue\n",
    "                    #print tds[0]\n",
    "                    if tds[0].find('a'):\n",
    "                        link_text =  tds[0].find('a')['href']\n",
    "                        if link_text.find(final_str) != -1:\n",
    "                            listA_runs.append(tds[3].text)\n",
    "                            listA_wkt.append(tds[4].text)\n",
    "                            listA_overs.append(tds[1].text)\n",
    "                            listA_date.append(str(date.date()))\n",
    "                            break\n",
    "                            #print tds[2].text\n",
    "                            #print tds[3].text\n",
    "        print 'work done for odis, now for t20s'\n",
    "\n",
    "\n",
    "\n",
    "        page = opener.open(t20_url)\n",
    "        soup = BeautifulSoup(page.read(),\"html.parser\")\n",
    "        table = soup.findAll('table')\n",
    "        rows =  table[0].findAll('tr')\n",
    "        #print rows[0]\n",
    "        date_start  = '2015-02-04'\n",
    "        date_end  = '2016-02-04'\n",
    "\n",
    "        for row in rows:\n",
    "            tds  = row.findAll('td')\n",
    "            #print tds[1].text\n",
    "            date  = parser.parse(str(tds[1].text))\n",
    "            if date > parser.parse(date_start) and date < parser.parse(date_end):\n",
    "                print date.date()\n",
    "                #t20_date.append(str(date.date()))\n",
    "                mlink = tds[4].find('a')\n",
    "                match_link = 'http://cricketarchive.com/' + mlink['href']\n",
    "                print match_link\n",
    "                match_page = opener.open(match_link)\n",
    "                match_soup = BeautifulSoup(match_page.read(),\"html.parser\")\n",
    "                #print match_soup\n",
    "\n",
    "                found  = 0\n",
    "                tabs = match_soup.findAll('table')\n",
    "\n",
    "\n",
    "                rows = tabs[2].findAll('tr')\n",
    "                for row in rows:\n",
    "                    tds  = row.findAll('td')\n",
    "                    if len(tds) < 4:\n",
    "                        continue\n",
    "                    #print tds[0]\n",
    "                    if tds[0].find('a'):\n",
    "                        link_text =  tds[0].find('a')['href']\n",
    "                        if link_text.find(final_str) != -1:\n",
    "                            #print tds[2].text\n",
    "                            #print tds[3].text\n",
    "                            t20_runs.append(tds[3].text)\n",
    "                            t20_wkt.append(tds[4].text)\n",
    "                            t20_overs.append(tds[1].text)\n",
    "                            found =  1\n",
    "                            t20_date.append(str(date.date()))\n",
    "                            break\n",
    "                if (found== 1 ):\n",
    "                    continue\n",
    "\n",
    "\n",
    "                rows = tabs[4].findAll('tr')\n",
    "                for row in rows:\n",
    "                    tds  = row.findAll('td')\n",
    "                    #print tds[0]\n",
    "                    if len(tds)<4:\n",
    "                        continue\n",
    "                    if tds[0].find('a'):\n",
    "                        link_text =  tds[0].find('a')['href']\n",
    "                        if link_text.find(final_str) != -1:\n",
    "                            t20_runs.append(tds[3].text)\n",
    "                            t20_wkt.append(tds[4].text)\n",
    "                            t20_overs.append(tds[1].text)\n",
    "                            found =  1\n",
    "                            t20_date.append(str(date.date()))\n",
    "                            break\n",
    "                            #print tds[2].text\n",
    "                            #print tds[3].text\n",
    "        print 't20s done'\n",
    "\n",
    "        listA = pd.DataFrame()\n",
    "        listA['date'] = listA_date\n",
    "        listA['runs'] = listA_runs\n",
    "        listA['balls'] = listA_balls\n",
    "        fname = 'score_files/2016_listA_' + str(player_name) + '.csv'\n",
    "        listA.to_csv(fname, encoding='utf-8')\n",
    "\n",
    "        t20 = pd.DataFrame()\n",
    "        t20['date'] = t20_date\n",
    "        t20['runs'] = t20_runs\n",
    "        t20['balls'] = t20_balls\n",
    "        fname = 'score_files/2016_t20_' + str(player_name) + '.csv'\n",
    "        t20.to_csv(fname, encoding='utf-8')\n",
    "\n",
    "    except:\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
